// Подключаем заголовочный файл MPI для использования функций распределённых вычислений
#include <mpi.h>

// Подключаем стандартную библиотеку для ввода и вывода данных
#include <iostream>

// Подключаем стандартную библиотеку для работы с динамическими массивами
#include <vector>

// Подключаем библиотеку для измерения времени выполнения
#include <chrono>

// Объявляем точку входа в программу
int main(int argc, char** argv)
{
    // Инициализируем среду выполнения MPI
    MPI_Init(&argc, &argv);

    // Объявляем переменную для хранения номера текущего процесса
    int rank;

    // Получаем номер текущего процесса в группе MPI
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    // Объявляем переменную для хранения общего количества процессов
    int size;

    // Получаем общее количество процессов в группе MPI
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // Задаём общий размер обрабатываемого массива
    const int N = 1'000'000;

    // Вычисляем размер части массива, приходящейся на каждый процесс
    int local_N = N / size;

    // Объявляем вектор для хранения полного массива (используется только процессом 0)
    std::vector<float> data;

    // Объявляем вектор для хранения локальной части массива в каждом процессе
    std::vector<float> local_data(local_N);

    // Проверяем, является ли текущий процесс нулевым
    if (rank == 0)
    {
        // Выделяем память под полный массив и инициализируем его единицами
        data.resize(N, 1.0f);
    }

    // Выполняем синхронизацию всех процессов перед началом измерения времени
    MPI_Barrier(MPI_COMM_WORLD);

    // Фиксируем время начала выполнения распределённого алгоритма
    auto t1 = std::chrono::high_resolution_clock::now();

    // Распределяем части массива от процесса 0 ко всем процессам
    MPI_Scatter(
        data.data(),
        local_N,
        MPI_FLOAT,
        local_data.data(),
        local_N,
        MPI_FLOAT,
        0,
        MPI_COMM_WORLD
    );

    // Объявляем переменную для хранения локальной суммы текущего процесса
    float local_sum = 0.0f;

    // Последовательно вычисляем сумму элементов локальной части массива
    for (int i = 0; i < local_N; ++i)
        local_sum += local_data[i];

    // Объявляем переменную для хранения глобальной суммы
    float global_sum = 0.0f;

    // Выполняем сбор всех локальных сумм в одну глобальную сумму на процессе 0
    MPI_Reduce(
        &local_sum,
        &global_sum,
        1,
        MPI_FLOAT,
        MPI_SUM,
        0,
        MPI_COMM_WORLD
    );

    // Выполняем синхронизацию всех процессов перед завершением измерения времени
    MPI_Barrier(MPI_COMM_WORLD);

    // Фиксируем время окончания выполнения распределённого алгоритма
    auto t2 = std::chrono::high_resolution_clock::now();

    // Проверяем, является ли текущий процесс нулевым
    if (rank == 0)
    {
        // Вычисляем общее время выполнения в миллисекундах
        double ms = std::chrono::duration<double, std::milli>(t2 - t1).count();

        // Выводим заголовок задания
        std::cout << "Task 4: MPI Reduce\n";

        // Выводим количество используемых процессов
        std::cout << "Processes: " << size << "\n";

        // Выводим размер обрабатываемого массива
        std::cout << "Array size: " << N << "\n";

        // Выводим итоговую сумму элементов массива
        std::cout << "Result sum = " << global_sum << "\n";

        // Выводим время выполнения распределённого алгоритма
        std::cout << "Time = " << ms << " ms\n";
    }

    // Корректно завершаем работу среды MPI
    MPI_Finalize();

    // Возвращаем код успешного завершения программы
    return 0;
}
