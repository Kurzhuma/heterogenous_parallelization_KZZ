# Assignment 4 - Гибридные и распределенные параллельные вычисления

## Цель работы

Целью данной работы является изучение подходов к гибридным и распределенным параллельным вычислениям с использованием технологий CUDA и MPI, а также анализ эффективности различных способов организации вычислений: на CPU, на GPU, в гибридном режиме CPU+GPU и в распределенной среде с несколькими процессами.

---

## Структура работы

Работа состоит из четырех независимых заданий:

1. CUDA-редукция суммы массива с использованием глобальной памяти.
2. CUDA-реализация префиксной суммы (scan) с использованием разделяемой памяти.
3. Гибридная реализация обработки массива с разделением нагрузки между CPU и GPU.
4. Распределенная реализация с использованием MPI.

---

# Задание 1
## CUDA Reduce (глобальная память)

### Условие
Реализовать CUDA-программу для вычисления суммы элементов массива размером 100 000 с использованием глобальной памяти и сравнить с CPU-реализацией.

### Результаты

Array size: 100000

CPU sum = 100000, time = 0.2089 ms

GPU sum = 100000, time = 1.05677 ms


### Анализ выводов

GPU-версия оказалась медленнее CPU. Это объясняется следующими причинами:
- Используется `atomicAdd` в глобальной памяти, что создает сильную сериализацию операций.
- Накладные расходы на запуск CUDA-ядра и синхронизацию превышают выигрыш от параллелизма на таком небольшом объеме данных.
- CPU выполняет простую линейную сумму очень эффективно за счет кэширования и высокой тактовой частоты.

Вывод: **наивная редукция через global + atomicAdd неэффективна**.

---

# Задание 2
## CUDA Prefix Sum (Shared Memory)

### Условие
Реализовать префиксную сумму (scan) массива размером 1 000 000 с использованием разделяемой памяти и сравнить с CPU.

### Результаты

Array size: 1000000

CPU time = 3.8499 ms

GPU time = 2.19622 ms

Last element CPU = 1e+06

Last element GPU = 1e+06


### Анализ выводов

GPU-версия оказалась быстрее CPU, так как:
- Основная работа выполняется в **shared memory**, что значительно снижает задержки доступа.
- Вычисления внутри блоков хорошо распараллелены.
- CPU-реализация остается полностью последовательной.

Вывод: **использование shared memory дает реальное ускорение** для алгоритмов сканирования.

---

# Задание 3
## Гибридная обработка CPU + GPU

### Условие
Половина массива обрабатывается на CPU, вторая половина — на GPU. Сравниваются три режима:
- только CPU,
- только GPU,
- гибрид CPU + GPU.

### Результаты

Array size: 1000000

CPU only: sum = 1e+06, time = 2.1339 ms

GPU only: sum = 1e+06, time = 3.01466 ms

HYBRID: sum = 1e+06, time = 2.1747 ms


### Анализ выводов

- GPU-версия снова проигрывает CPU из-за использования `atomicAdd` и накладных расходов.
- Гибридная версия практически равна CPU по времени, так как:
    - CPU выполняет половину работы очень быстро,
    - GPU часть не дает существенного ускорения,
    - добавляются накладные расходы на копирование данных и синхронизацию.

Вывод: **гибридный подход эффективен только тогда, когда GPU-часть действительно ускоряет вычисления**.

---

# Задание 4
## MPI Reduce (распределенные вычисления)

### Условие
Реализовать распределенную версию суммирования массива размером 1 000 000 с использованием MPI и измерить время для 1, 2, 4 и 8 процессов.

### Результаты

Processes: 1 → 3.96 ms

Processes: 2 → 2.33 ms

Processes: 4 → 1.73 ms

Processes: 8 → 1.52 ms

### Анализ выводов

- При увеличении числа процессов время выполнения уменьшается.
- Однако ускорение **не линейное**, так как:
    - присутствуют накладные расходы на MPI_Scatter и MPI_Reduce,
    - растет стоимость синхронизации и обмена данными.

Вывод: **распределенные вычисления масштабируются, но ограничены коммуникационными издержками**.

---

# Общие выводы по работе

1. GPU не всегда быстрее CPU — эффективность зависит от алгоритма и структуры памяти.
2. Использование shared memory принципиально важно для высокой производительности на GPU.
3. Гибридные вычисления имеют смысл только при правильно сбалансированной нагрузке.
4. MPI позволяет эффективно масштабировать вычисления, но упирается в задержки обмена данными.

---

# Ответы на контрольные вопросы

---

## 1. В чем заключается отличие гибридных вычислений от вычислений только на CPU или только на GPU?

Гибридные вычисления предполагают **одновременное использование CPU и GPU** для решения одной задачи, при этом работа распределяется между ними. В отличие от CPU-only или GPU-only подходов, гибридная модель позволяет задействовать сильные стороны обеих архитектур, однако требует явного управления разделением данных, их передачей и синхронизацией между устройствами.

---

## 2. Для каких типов задач целесообразно распределять вычисления между CPU и GPU?

Распределение вычислений между CPU и GPU целесообразно для задач, которые:
- работают с большими объемами данных,
- хорошо параллелятся,
- допускают разбиение на независимые части,
- имеют высокую вычислительную сложность по сравнению с накладными расходами на передачу данных между CPU и GPU.

---

## 3. В чем разница между синхронной и асинхронной передачей данных между CPU и GPU?

Синхронная передача данных блокирует выполнение программы до полного завершения операции копирования. Асинхронная передача позволяет запускать вычисления параллельно с копированием данных, не дожидаясь его завершения, при наличии соответствующих механизмов синхронизации.

---

## 4. Почему асинхронная передача данных может повысить производительность программы?

Асинхронная передача данных позволяет **перекрывать вычисления и обмен данными**, благодаря чему уменьшается время простоя CPU и GPU. В результате общее время выполнения программы может быть существенно сокращено.

---

## 5. Какие основные функции MPI используются для распределения и сбора данных между процессами?

В реализованной программе и в типичных MPI-приложениях используются следующие основные функции:
- `MPI_Init` и `MPI_Finalize` — инициализация и завершение работы MPI,
- `MPI_Comm_size` — получение количества процессов,
- `MPI_Comm_rank` — получение номера текущего процесса,
- `MPI_Scatter` — распределение частей массива между процессами,
- `MPI_Reduce` — сбор и агрегация результатов,
- `MPI_Barrier` — синхронизация всех процессов.

---

## 6. Как количество процессов MPI влияет на время выполнения программы и почему?

При увеличении числа процессов вычислительная нагрузка распределяется между большим количеством исполнителей, что уменьшает время вычислений на каждом процессе. Однако при этом растут накладные расходы на обмен данными и синхронизацию, поэтому ускорение не является линейным и со временем начинает замедляться.

---

## 7. Какие факторы ограничивают масштабируемость распределенных параллельных программ?

Основными ограничивающими факторами являются:
- задержки и пропускная способность сети,
- накладные расходы на передачу данных между процессами,
- необходимость синхронизации,
- неравномерное распределение нагрузки,
- ограничения по памяти и архитектуре системы.

---

## 8. В каких случаях использование распределенных вычислений оправдано, а в каких — неэффективно?

Использование распределенных вычислений оправдано для:
- очень больших объемов данных,
- вычислительно тяжелых задач,
- задач, хорошо масштабируемых по числу процессов.

Распределенные вычисления неэффективны для:
- малых задач,
- задач с частыми синхронизациями,
- случаев, когда стоимость передачи данных превышает стоимость самих вычислений.

---
