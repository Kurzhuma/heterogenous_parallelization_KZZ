# Практическая работа №5
## Тема: Реализация параллельных структур данных на GPU

---

## Цель работы

1. Освоить программирование параллельных структур данных с использованием CUDA.
2. Реализовать параллельные структуры данных (стек и очередь).
3. Провести исследование производительности реализованных структур данных.

---

## Теоретические сведения

Параллельные структуры данных — это структуры, которые поддерживают безопасный доступ к данным несколькими потоками одновременно. Их реализация требует использования механизмов синхронизации и атомарных операций, так как при одновременном доступе нескольких потоков возможны конфликты записи и чтения.

В данной работе рассматриваются две структуры данных:

- Стек (LIFO) — добавление и удаление элементов происходит с одного конца.
- Очередь (FIFO) — добавление происходит в конец, а удаление — из начала.

Для реализации используется платформа CUDA, позволяющая запускать тысячи потоков параллельно на графическом процессоре.

---

## Описание реализации

### Параллельный стек

Стек реализован в виде структуры, содержащей:

- массив данных в глобальной памяти GPU,
- переменную `top`, указывающую на вершину стека,
- переменную `capacity`, задающую максимальный размер.

Операции:

- `push` — выполняется с помощью `atomicAdd(&top, 1)`, что гарантирует, что каждый поток получает уникальную позицию в стеке.
- `pop` — выполняется с помощью `atomicSub(&top, 1)`.

Для тестирования реализованы CUDA-ядра, в которых тысячи потоков параллельно выполняют операции добавления и извлечения элементов.

---

### Параллельная очередь (MPMC)

Очередь реализована как кольцевой буфер (ring buffer) с поддержкой нескольких производителей и нескольких потребителей (MPMC):

- используется массив в глобальной памяти,
- два атомарных счётчика: `head` и `tail`,
- индекс вычисляется по модулю `capacity`.

Операции:

- `enqueue` — атомарно увеличивает `tail`,
- `dequeue` — атомарно увеличивает `head`.

Для демонстрации оптимизации в ядре добавления используется разделяемая память (`shared memory`) как промежуточный буфер.

---

### Сравнение с CPU

Для сравнения реализованы последовательные версии стека и очереди на CPU с использованием `std::vector`. Время измеряется с помощью `std::chrono`.

---

## Результаты экспериментов

Вывод программы:

N = 100000

GPU Stack push: 18.8508 ms

GPU Stack pop: 0.022528 ms

GPU Queue enqueue: 0.022528 ms

GPU Queue dequeue: 0.104448 ms

N = 1000000

GPU Stack push: 0.053248 ms

GPU Stack pop: 0.032544 ms

GPU Queue enqueue: 0.054272 ms

GPU Queue dequeue: 0.05632 ms

CPU Stack total: 50.0746 ms

CPU Queue total: 163205 ms


---

## Анализ результатов

1. Операция `push` для стека на GPU при размере 100000 работает значительно медленнее, чем остальные операции. Это объясняется тем, что все потоки конкурируют за одну атомарную переменную `top`. В результате доступ к ней сериализуется, и параллельный алгоритм фактически становится почти последовательным.

2. Операции `pop` для стека и операции очереди (`enqueue` и `dequeue`) выполняются значительно быстрее, так как нагрузка распределяется между разными счётчиками (`head` и `tail`) и степень конфликтов ниже.

3. Для размера 1 000 000 элементов время выполнения на GPU становится очень малым, что связано с высокой пропускной способностью GPU и тем, что измеряется только время выполнения ядра.

4. Последовательная очередь на CPU работает крайне медленно, так как используется операция `erase(begin())`, имеющая линейную сложность. Это наглядно демонстрирует, насколько важен правильный выбор структуры данных.

5. В целом эксперимент показывает, что параллельные структуры данных могут обеспечивать высокую производительность, но при неудачном проектировании (как в случае стека с одним счётчиком) атомарные операции становятся узким местом.

---

## Выводы

В ходе работы были реализованы:

- параллельный стек на CUDA,
- параллельная очередь с поддержкой нескольких производителей и потребителей (MPMC),
- вариант с использованием разделяемой памяти,
- выполнено сравнение с последовательными версиями на CPU.

Эксперименты показали, что:

- атомарные операции обеспечивают корректность, но могут существенно снижать производительность,
- конкуренция потоков за одну переменную является серьёзным узким местом,
- GPU может значительно превосходить CPU, но только при правильной организации алгоритма и памяти.

---

## Ответы на контрольные вопросы

### 1. В чём отличие стека и очереди?

Стек работает по принципу LIFO (последний вошёл — первый вышел). Очередь работает по принципу FIFO (первый вошёл — первый вышел).

---

### 2. Какие проблемы возникают при параллельном доступе к данным?

Основные проблемы — это состояния гонки (race conditions), когда несколько потоков одновременно пытаются читать и записывать одни и те же данные, что приводит к повреждению структуры данных.

---

### 3. Как атомарные операции помогают избежать конфликтов?

Атомарные операции гарантируют, что операция чтения и изменения переменной выполняется как неделимое целое. Это предотвращает ситуацию, когда два потока получают одно и то же значение счётчика и записывают данные в одно и то же место.

---

### 4. Какие типы памяти CUDA используются для хранения данных?

В данной работе используются:
- глобальная память — для хранения массивов данных и структур,
- разделяемая память — как временный буфер внутри блока потоков,
- регистры и локальная память — для временных переменных потоков.

---

### 5. Как синхронизация потоков влияет на производительность?

Синхронизация и атомарные операции приводят к сериализации выполнения потоков. Чем больше потоков конкурируют за один ресурс, тем сильнее падает производительность.

---

### 6. Почему разделяемая память важна для оптимизации работы параллельных структур данных?

Разделяемая память значительно быстрее глобальной. Использование её в качестве промежуточного буфера позволяет сократить количество обращений к глобальной памяти и повысить производительность.

---

## Дополнительные задания

В рамках работы дополнительно:

- реализована очередь с поддержкой нескольких производителей и потребителей (MPMC),
- использована разделяемая память для буферизации данных,
- выполнено сравнение производительности с последовательными версиями на CPU.

---

