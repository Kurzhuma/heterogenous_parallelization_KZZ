# Практическая работа №4

## Тема
Оптимизация параллельного кода на GPU с использованием различных типов памяти.

---

## Цель работы

Изучение и практическое применение различных типов памяти CUDA (глобальной, разделяемой и локальной) для оптимизации параллельных вычислений на графическом процессоре.

---

## Теоретическая часть

### 1. Типы памяти в CUDA

**Глобальная память** — доступна всем потокам, обладает большим объёмом, но характеризуется высокой латентностью. Используется для хранения основных массивов данных.

**Разделяемая память (shared memory)** — быстрая память, общая для потоков одного блока. Эффективна для обмена данными и выполнения коллективных операций внутри блока.

**Локальная память** — фактически размещается в регистрах и локальной памяти потока. Доступна только одному потоку, обладает высокой скоростью, но сильно ограничена по объёму.

---

### 2. Оптимизация с использованием памяти

- Минимизация обращений к глобальной памяти за счёт использования разделяемой памяти.
- Использование глобальной памяти только для хранения исходных и результирующих данных.
- Использование локальной памяти для временных переменных и обработки малых подмассивов.

---

### 3. Пример использования

В качестве примера рассматривается задача параллельного нахождения суммы элементов массива.  
Глобальная память используется для хранения исходного массива, а разделяемая память — для промежуточных частичных сумм внутри блоков.

---

## Практическая часть

### Задания

В рамках работы были реализованы следующие этапы:

1. **Подготовка данных**
    - Реализована генерация массива случайных чисел размером до 1 000 000 элементов.

2. **Оптимизация параллельного редукционного алгоритма**
    - Реализована редукция суммы элементов массива:
        - а) с использованием только глобальной памяти (через atomicAdd);
        - б) с использованием комбинации глобальной и разделяемой памяти (суммирование внутри блока + atomicAdd).
    - Выполнено сравнение производительности двух подходов.

3. **Оптимизация сортировки на GPU**
    - Реализована пузырьковая сортировка небольших подмассивов с использованием локальной памяти.
    - Основной массив хранится в глобальной памяти.
    - Реализовано слияние отсортированных подмассивов с использованием разделяемой памяти.

4. **Измерение производительности**
    - Выполнены замеры времени выполнения для массивов размером:
        - 10 000
        - 100 000
        - 1 000 000 элементов
    - На основе полученных данных предполагается построение графиков зависимости времени выполнения от размера массива.

---

## Результаты экспериментов

Результаты измерений времени выполнения (в миллисекундах):

### Размер массива: 10 000

GPU Reduce GLOBAL: 1.62509 ms

GPU Reduce SHARED: 0.105472 ms

GPU Local Bubble Sort: 0.227296 ms

GPU Merge (shared): 0.088064 ms

### Размер массива: 100 000

GPU Reduce GLOBAL: 0.172032 ms

GPU Reduce SHARED: 0.045056 ms

GPU Local Bubble Sort: 0.056864 ms

GPU Merge (shared): 0.034784 ms

### Размер массива: 1 000 000

GPU Reduce GLOBAL: 1.62918 ms

GPU Reduce SHARED: 0.053248 ms

GPU Local Bubble Sort: 0.217856 ms

GPU Merge (shared): 0.131072 ms


---

## Анализ результатов

Использование разделяемой памяти в задаче редукции позволяет сократить количество обращений к глобальной памяти и существенно уменьшить число атомарных операций. Это приводит к ускорению выполнения более чем на порядок по сравнению с вариантом, использующим только глобальную память.

В задаче сортировки локальная память эффективно применяется для обработки малых подмассивов, а разделяемая память — для операций слияния. Глобальная память используется только для хранения исходных и результирующих данных.

Для малых размеров массивов влияние накладных расходов на запуск CUDA-ядер и эффектов кэширования может доминировать над вычислительной сложностью алгоритмов, что объясняет нелинейный характер изменения времени выполнения.

---

## Выводы

В ходе работы было показано, что:

- Использование разделяемой памяти позволяет существенно повысить производительность параллельных алгоритмов на GPU.
- Минимизация обращений к глобальной памяти является ключевым фактором оптимизации CUDA-программ.
- Локальная память эффективна для обработки небольших локальных фрагментов данных.
- Рациональное комбинирование различных типов памяти позволяет добиться значительного ускорения вычислений.

---

## Контрольные вопросы и ответы

### 1. Чем отличаются типы памяти в CUDA и в каких случаях их использовать?

Глобальная память используется для хранения больших массивов данных, но имеет высокую латентность. Разделяемая память применяется для обмена данными внутри блока и коллективных операций. Локальная память используется для временных переменных и обработки небольших подзадач внутри одного потока.

---

### 2. Как использование разделяемой памяти влияет на производительность?

Разделяемая память значительно быстрее глобальной. Её использование позволяет уменьшить количество обращений к глобальной памяти и тем самым существенно повысить производительность.

---

### 3. Доступ и как его обеспечить?

Под доступом в контексте CUDA понимается возможность корректного и безопасного обращения потоков к данным, размещённым в различных типах памяти (глобальной, разделяемой и локальной).

Корректный доступ к памяти обеспечивается следующими мерами:

- правильным размещением данных по типам памяти в зависимости от их назначения (глобальная память — для общих массивов данных, разделяемая — для совместно используемых внутри блока данных, локальная — для временных переменных потока);
- корректным вычислением индексов потоков и обязательной проверкой выхода за границы массивов;
- использованием синхронизации потоков при работе с разделяемой памятью (например, с помощью `__syncthreads()`), чтобы избежать состояния гонки;
- предварительной инициализацией памяти и контролем ошибок вызовов CUDA API.

Таким образом, доступ к памяти обеспечивается за счёт правильной организации структуры данных, корректной индексации потоков и применения механизмов синхронизации.

---

### 4. Какие сложности возникают при работе с большим объёмом данных на GPU?

Основные сложности связаны с ограниченным объёмом видеопамяти, высокой латентностью глобальной памяти и необходимостью минимизировать пересылки данных между CPU и GPU.

---

### 5. Почему важно минимизировать доступ к глобальной памяти?

Потому что глобальная память обладает высокой латентностью, и частые обращения к ней существенно замедляют выполнение программы.

---

### 6. Как использовать профилирование для анализа производительности CUDA-программ?

Для этого применяются инструменты NVIDIA (Nsight, nvprof, Nsight Compute), которые позволяют анализировать время выполнения ядер, загрузку памяти, количество обращений к различным типам памяти и выявлять узкие места программы.

---
