# Практическая работа №9

Kurmash Zhumagozhayev ADA-2401M

**Тема:** Распределенная обработка данных с использованием MPI

---

## Цель работы

- Изучение продвинутых коллективных операций MPI (`MPI_Scatterv`, `MPI_Reduce`, `MPI_Bcast`).
- Реализация распределенных алгоритмов для статистического анализа, линейной алгебры и анализа графов.
- Исследование производительности и масштабируемости распределенных приложений.

---

## Описание работы

В рамках данной работы реализованы три вычислительных модуля:

1. **Статистический анализ:** Расчет среднего значения и стандартного отклонения массива из $10^6$ элементов.
2. **Линейная алгебра:** Прямой ход метода Гаусса для решения систем линейных уравнений.
3. **Графовые алгоритмы:** Поиск кратчайших путей между всеми парами вершин методом Флойда-Уоршелла.

---

## Структура проекта

- `main.cpp` — реализация вычислительных ядер и логики управления процессами.
- `mpi_utils.h` — вспомогательные функции для генерации матриц и векторов.
- `CMakeLists.txt` — конфигурация сборки с поддержкой MPI.

---

## Реализованные задачи

### Задача 1: Распределенный расчет статистики
- Использована функция **`MPI_Scatterv`**, что позволило корректно распределить данные между процессами при любом их количестве (обработка остатка).
- Локальные суммы и суммы квадратов агрегированы на процессе `rank 0` через **`MPI_Reduce`**.

### Задача 2: Распределенный метод Гаусса
- Матрица распределена построчно.
- Реализована рассылка ведущей строки через **`MPI_Bcast`** на каждой итерации, что исключило избыточные пересылки данных.

### Задача 3: Алгоритм Флойда-Уоршелла
- Параллельная реализация поиска кратчайших путей в графе.
- Синхронизация данных о промежуточных вершинах обеспечена через коллективное вещание.

---

## Результаты исследования производительности (Задание 4)

| Количество процессов (-n) | Статистика (сек) | Метод Гаусса (сек) | Алгоритм Флойда (сек) |
|:--------------------------| :---: | :---: | :---: |
| 1 (Референс)              | 0.0339 | 0.1678 | 0.8888 |
| 2                         | 0.0224 | 0.1011 | 0.4094 |
| 4                         | 0.0178 | 0.0556 | 0.2035 |

### Анализ масштабируемости
1. **Алгоритм Флойда-Уоршелла:** Продемонстрировал наилучшую масштабируемость. При использовании 4 процессов время сократилось более чем в 4 раза. Это доказывает, что вычислительная сложность задачи ($O(N^3)$) полностью перекрывает коммуникационные затраты MPI.
2. **Метод Гаусса:** Также показал стабильное ускорение (около 3x на 4 процессах).
3. **Статистика:** Ускорение менее выражено (с 0.03с до 0.017с). Это подтверждает теорию: для простых линейных задач ($O(N)$) накладные расходы на распределение данных через `Scatterv` и сбор через `Reduce` сопоставимы со временем самих вычислений.

---

## Выводы

- Распределенные вычисления на базе MPI позволяют кратно сократить время решения тяжелых вычислительных задач.
- Эффективность параллелизма напрямую зависит от сложности алгоритма: чем выше вычислительная плотность, тем лучше масштабируемость.
- Коллективные операции (`Bcast`, `Scatterv`) обеспечивают синхронную работу узлов и минимизируют простои системы.

---

## Ответы на контрольные вопросы

### 1. Как изменяется время выполнения при увеличении числа процессов?
При увеличении числа процессов время выполнения сокращается (ускорение), пока объем вычислений на каждом узле существенно превышает время, затрачиваемое на пересылку данных по сети.

### 2. Какие факторы могут влиять на производительность программы?
- Пропускная способность и задержки (latency) сети.
- Балансировка нагрузки (равномерность распределения строк/элементов).
- Накладные расходы на инициализацию среды MPI.

### 3. Как можно оптимизировать передачу данных между процессами?
- Использовать коллективные операции вместо циклов с `MPI_Send`.
- Группировать данные в более крупные блоки перед отправкой.
- Совмещать вычисления с передачей данных (неблокирующие операции).

### 4. Какие ограничения возникают при работе с большими данными?
Основное ограничение — объем оперативной памяти на одном узле (процесс `rank 0` должен сначала создать весь массив) и возрастающее время синхронизации всех процессов.