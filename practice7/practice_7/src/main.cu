/**
 * ПРАКТИЧЕСКАЯ РАБОТА №7:
 * Задачи: Редукция и Сканирование (Blelloch) для массивов до 10^7 элементов.
 */

// Подключение заголовочного файла с вспомогательными утилитами CUDA
#include "utils.h"
// Подключение библиотеки OpenMP для замера временных интервалов
#include <omp.h>
// Подключение функций Windows API для управления кодировкой консоли
#include <windows.h>
// Подключение стандартной библиотеки математических функций
#include <math.h>

// Определение константы размера блока потоков для графического процессора
#define BS 256

// ============================================================================
// CPU РЕАЛИЗАЦИИ ДЛЯ АНАЛИЗА ПРОИЗВОДИТЕЛЬНОСТИ
// ============================================================================

/**
 * Последовательная редукция (сумма) на CPU
 */
// Объявление функции последовательного суммирования элементов массива на хосте
float cpu_reduce(float* data, size_t n) {
    // Инициализация переменной для хранения накопленной суммы
    float sum = 0.0f;
    // Последовательный перебор всех элементов массива в цикле
    for (size_t i = 0; i < n; i++) sum += data[i];
    // Возврат итогового результата вычислений
    return sum;
}

/**
 * Последовательное эксклюзивное сканирование на CPU
 */
// Объявление функции расчета эксклюзивной префиксной суммы на хосте
void cpu_scan(float* in, float* out, size_t n) {
    // Установка первого элемента выходного массива в нулевое значение (эксклюзивный скан)
    out[0] = 0;
    // Итеративное вычисление суммы элементов на основе предыдущего результата
    for (size_t i = 1; i < n; i++) {
        // Текущее значение префикса равно сумме предыдущего префикса и входного значения
        out[i] = out[i - 1] + in[i - 1];
    }
}

// ============================================================================
// GPU ЯДРА (CUDA)
// ============================================================================

/**
 * Задание 1: Ядро редукции с использованием разделяемой памяти
 */
// Определение CUDA-ядра для параллельного суммирования элементов массива
__global__ void reduceKernel(float* in, float* out, size_t n) {
    // Объявление массива в разделяемой памяти блока для промежуточного хранения данных
    __shared__ float sdata[BS];
    // Получение локального идентификатора потока внутри блока
    int tid = threadIdx.x;
    // Вычисление глобального индекса обрабатываемого элемента в сетке
    size_t i = blockIdx.x * blockDim.x + threadIdx.x;

    // Кооперативная загрузка данных из глобальной видеопамяти в разделяемую память
    sdata[tid] = (i < n) ? in[i] : 0.0f;
    // Синхронизация потоков внутри блока перед началом редукции
    __syncthreads();

    // Реализация древовидного алгоритма суммирования внутри разделяемой памяти
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        // Условие активности потока на текущем уровне дерева редукции
        if (tid < s) {
            // Суммирование элементов с учетом текущего шага смещения
            sdata[tid] += sdata[tid + s];
        }
        // Ожидание завершения этапа суммирования всеми потоками блока
        __syncthreads();
    }

    // Использование атомарной операции сложения первым потоком блока для записи в итоговый результат
    if (tid == 0) atomicAdd(out, sdata[0]);
}

/**
 * Задание 2: Ядро сканирования Блеллока (Up-sweep фаза)
 */
// Определение CUDA-ядра для реализации алгоритма сканирования Блеллока (включая Up-sweep и Down-sweep)
__global__ void scanUpSweep(float* d_data, float* d_blockSums, size_t n) {
    // Указание на использование динамической разделяемой памяти
    extern __shared__ float temp[];
    // Получение локального индекса потока
    int tid = threadIdx.x;
    // Инициализация начального шага для алгоритма сканирования
    size_t offset = 1;
    // Расчет первого локального индекса пары для текущего потока
    size_t ai = tid;
    // Расчет второго локального индекса пары для текущего потока
    size_t bi = tid + (BS / 2);
    // Определение глобального индекса для первого элемента пары
    size_t global_ai = blockIdx.x * BS + ai;
    // Определение глобального индекса для второго элемента пары
    size_t global_bi = blockIdx.x * BS + bi;

    // Загрузка первой части данных в разделяемую память блока
    temp[ai] = (global_ai < n) ? d_data[global_ai] : 0;
    // Загрузка второй части данных в разделяемую память блока
    temp[bi] = (global_bi < n) ? d_data[global_bi] : 0;

    // Реализация фазы восходящего обхода (Up-sweep) для построения дерева частичных сумм
    for (int d = BS >> 1; d > 0; d >>= 1) {
        // Барьерная синхронизация потоков перед операциями на текущем уровне
        __syncthreads();
        // Проверка активности потока на текущем шаге фазы Up-sweep
        if (tid < d) {
            // Расчет индексов элементов пары для сложения
            int i1 = offset * (2 * tid + 1) - 1;
            int i2 = offset * (2 * tid + 2) - 1;
            // Выполнение сложения на текущем уровне дерева
            temp[i2] += temp[i1];
        }
        // Увеличение шага смещения для следующего уровня иерархии
        offset *= 2;
    }

    // Обработка последнего элемента блока первым потоком
    if (tid == 0) {
        // Сохранение полной суммы текущего блока для коррекции последующих блоков
        d_blockSums[blockIdx.x] = temp[BS - 1];
        // Обнуление последнего элемента для реализации эксклюзивного сканирования
        temp[BS - 1] = 0;
    }

    // Реализация фазы нисходящего обхода (Down-sweep) для распределения префиксных сумм
    for (int d = 1; d < BS; d *= 2) {
        // Уменьшение шага смещения для спуска по уровням дерева
        offset >>= 1;
        // Барьерная синхронизация потоков блока
        __syncthreads();
        // Проверка активности потока на текущем шаге фазы Down-sweep
        if (tid < d) {
            // Определение индексов в разделяемой памяти для обмена и сложения
            int i1 = offset * (2 * tid + 1) - 1;
            int i2 = offset * (2 * tid + 2) - 1;
            // Временное сохранение значения левого потомка
            float t = temp[i1];
            // Передача текущего значения префикса левому потомку
            temp[i1] = temp[i2];
            // Вычисление префиксной суммы для правого потомка
            temp[i2] += t;
        }
    }
    // Финальная синхронизация перед выгрузкой результатов из разделяемой памяти
    __syncthreads();

    // Запись вычисленного эксклюзивного префикса первого элемента в глобальную видеопамять
    if (global_ai < n) d_data[global_ai] = temp[ai];
    // Запись вычисленного эксклюзивного префикса второго элемента в глобальную видеопамять
    if (global_bi < n) d_data[global_bi] = temp[bi];
}

/**
 * Финальная коррекция: добавление префиксных сумм предыдущих блоков
 */
// Определение CUDA-ядра для объединения результатов локальных сканирований в глобальный массив
__global__ void scanFinalAdd(float* d_data, float* d_blockPrefixes, size_t n) {
    // Расчет глобального индекса потока в сетке
    size_t i = blockIdx.x * BS + threadIdx.x;
    // Условие добавления смещения ко всем блокам, кроме первого
    if (i < n && blockIdx.x > 0) {
        // Прибавление накопленной суммы всех предыдущих блоков к текущему элементу
        d_data[i] += d_blockPrefixes[blockIdx.x];
    }
}

// ============================================================================
// ФУНКЦИЯ ТЕСТИРОВАНИЯ
// ============================================================================

// Объявление управляющей функции для проведения сравнительного тестирования производительности
void run_test_suite(size_t n) {
    // Вычисление общего объема памяти в байтах для размещения данных
    size_t bytes = n * sizeof(float);
    // Выделение оперативной памяти на хосте для входных данных
    float *h_in = (float*)malloc(bytes);
    // Выделение оперативной памяти на хосте для результатов CPU
    float *h_out_cpu = (float*)malloc(bytes);
    // Инициализация входного массива единичными значениями
    for (size_t i = 0; i < n; i++) h_in[i] = 1.0f;

    // Объявление указателей для размещения данных в видеопамяти
    float *d_in, *d_out_reduce, *d_block_sums;
    // Резервирование глобальной видеопамяти под основной массив данных
    CHECK_CUDA(cudaMalloc(&d_in, bytes));
    // Резервирование видеопамяти под итоговый результат редукции
    CHECK_CUDA(cudaMalloc(&d_out_reduce, sizeof(float)));

    // Копирование исходных данных из памяти хоста в видеопамять устройства
    CHECK_CUDA(cudaMemcpy(d_in, h_in, bytes, cudaMemcpyHostToDevice));
    // Обнуление целевой ячейки памяти для результата суммирования
    CHECK_CUDA(cudaMemset(d_out_reduce, 0, sizeof(float)));

    // Вывод информации о параметрах текущего тестового запуска
    printf("\n=== ТЕСТ: %zu ЭЛЕМЕНТОВ ===\n", n);

    // --- РЕДУКЦИЯ ---
    // Фиксация времени начала выполнения операции на центральном процессоре
    double t1 = omp_get_wtime();
    // Выполнение последовательной редукции на хосте
    float res_cpu = cpu_reduce(h_in, n);
    // Фиксация времени завершения операции на хосте
    double t2 = omp_get_wtime();
    // Расчет длительности вычислений на CPU в миллисекундах
    double cpu_r_time = (t2 - t1) * 1000.0;

    // Фиксация времени начала выполнения операции на графическом процессоре
    t1 = omp_get_wtime();
    // Расчет необходимого количества блоков в сетке CUDA
    int blocks = (n + BS - 1) / BS;
    // Запуск CUDA-ядра редукции с заданной конфигурацией сетки
    reduceKernel<<<blocks, BS>>>(d_in, d_out_reduce, n);
    // Синхронизация центрального процессора с завершением выполнения всех задач на GPU
    cudaDeviceSynchronize();
    // Фиксация времени завершения выполнения ядра на устройстве
    t2 = omp_get_wtime();
    // Расчет длительности вычислений на GPU в миллисекундах
    double gpu_r_time = (t2 - t1) * 1000.0;

    // Вывод сравнительных характеристик производительности редукции
    printf("[REDUCE] CPU: %8.4f ms | GPU: %8.4f ms | Speedup: %.2fx\n", cpu_r_time, gpu_r_time, cpu_r_time / gpu_r_time);

    // --- СКАНИРОВАНИЕ ---
    // Замер времени начала последовательного сканирования на CPU
    t1 = omp_get_wtime();
    // Выполнение эксклюзивного сканирования на хосте
    cpu_scan(h_in, h_out_cpu, n);
    // Замер времени окончания последовательного сканирования
    t2 = omp_get_wtime();
    // Расчет длительности сканирования на центральном процессоре
    double cpu_s_time = (t2 - t1) * 1000.0;

    // Замер времени начала многоэтапного сканирования на GPU
    t1 = omp_get_wtime();
    // Определение количества блоков для обработки данных в режиме сканирования
    int scanBlocks = (n + BS - 1) / BS;
    // Резервирование видеопамяти для хранения промежуточных сумм блоков
    CHECK_CUDA(cudaMalloc(&d_block_sums, scanBlocks * sizeof(float)));

    // Выполнение первого этапа: расчет локальных префиксов внутри блоков
    scanUpSweep<<<scanBlocks, BS / 2, BS * sizeof(float)>>>(d_in, d_block_sums, n);

    // Проверка необходимости выполнения межблочной коррекции (если данных больше одного блока)
    if (scanBlocks > 1) {
        // Выделение памяти на хосте для переноса сумм блоков
        float *h_b_sums = (float*)malloc(scanBlocks * sizeof(float));
        // Чтение сумм блоков из видеопамяти для обработки на хосте
        CHECK_CUDA(cudaMemcpy(h_b_sums, d_block_sums, scanBlocks * sizeof(float), cudaMemcpyDeviceToHost));
        // Инициализация аккумулятора для расчета префиксов блоков
        float acc = 0;
        // Расчет эксклюзивной префиксной суммы для массива сумм блоков на CPU
        for(int j=0; j<scanBlocks; j++) { float t = h_b_sums[j]; h_b_sums[j] = acc; acc += t; }
        // Возврат вычисленных смещений блоков обратно в видеопамять
        CHECK_CUDA(cudaMemcpy(d_block_sums, h_b_sums, scanBlocks * sizeof(float), cudaMemcpyHostToDevice));
        // Запуск ядра финальной коррекции для добавления смещений к каждому элементу
        scanFinalAdd<<<scanBlocks, BS>>>(d_in, d_block_sums, n);
        // Освобождение временного массива сумм блоков на хосте
        free(h_b_sums);
    }
    // Синхронизация GPU перед фиксацией времени окончания всех этапов сканирования
    cudaDeviceSynchronize();
    // Фиксация времени окончания многопоточного сканирования
    t2 = omp_get_wtime();
    // Расчет суммарного времени сканирования на GPU
    double gpu_s_time = (t2 - t1) * 1000.0;

    // Вывод сравнительных характеристик производительности сканирования
    printf("[SCAN]   CPU: %8.4f ms | GPU: %8.4f ms | Speedup: %.2fx\n", cpu_s_time, gpu_s_time, cpu_s_time / gpu_s_time);

    // Освобождение ресурсов глобальной видеопамяти после завершения тестов
    cudaFree(d_in); cudaFree(d_out_reduce); cudaFree(d_block_sums);
    // Освобождение ресурсов оперативной памяти на стороне хоста
    free(h_in); free(h_out_cpu);
}

// Точка входа в программу
int main() {
    // Установка кодировки UTF-8 для корректного отображения символов кириллицы в консоли Windows
    SetConsoleOutputCP(65001);
    // Инициализация генератора случайных чисел фиксированным значением для воспроизводимости
    srand(42);

    // Выполнение серии тестов для малого объема данных (соответствует размеру блока)
    run_test_suite(1024);
    // Выполнение серии тестов для среднего объема данных (1 миллион элементов)
    run_test_suite(1000000);
    // Выполнение серии тестов для большого объема данных (10 миллионов элементов)
    run_test_suite(10000000);

    // Вывод уведомления об успешном завершении всех вычислительных процедур
    printf("\nРасчет завершен.\n");
    // Завершение выполнения программы с кодом 0
    return 0;
}