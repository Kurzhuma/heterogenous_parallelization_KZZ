# Assignment 3
**Тема:** Архитектура GPU и оптимизация CUDA-программ  
Kurmash Zhumagozhayev, ADA-2401M
---

## Содержание

1. Описание задания
2. Описание реализованной программы
    - Общая структура
    - Задание 1: Global vs Shared memory
    - Задание 2: Сложение массивов и размер блока
    - Задание 3: Коалесцированный и некоалесцированный доступ
    - Задание 4: Подбор конфигурации сетки и блоков
3. Организация и логика кода
4. Методика измерения времени
5. Анализ результатов
6. Ответы на контрольные вопросы

---

## 1. Описание задания

**Assignment 3** посвящен изучению архитектуры GPU и практических аспектов оптимизации CUDA-программ. Основной упор сделан на анализ:

- различных типов памяти (глобальная и разделяемая),
- влияния шаблона доступа к памяти,
- влияния размера блока потоков,
- влияния конфигурации сетки и блоков.

Во всех экспериментах используется массив размером **N = 1 000 000 элементов**.

---

## 2. Описание реализованной программы

### 2.1 Общая структура

Программа реализована на C++ с использованием CUDA Runtime API. Она содержит набор CUDA-ядер для всех четырех заданий, код выделения и копирования памяти между CPU и GPU, измерение времени выполнения с помощью CUDA Events и вывод результатов в консоль. Все вычисления выполняются над массивами типа `float`.

---

### 2.2 Задание 1: Global vs Shared memory

В программе реализованы два CUDA-ядра:

#### `multiply_global`

Ядро выполняет поэлементное умножение массива, используя только глобальную память. Каждый поток вычисляет свой глобальный индекс, считывает элемент из глобальной памяти, умножает его на константу и записывает результат обратно.

#### `multiply_shared`

В этом ядре используется разделяемая память `__shared__ float buffer[256]`. Сначала каждый поток копирует свой элемент из глобальной памяти в shared memory, затем выполняется умножение, и после синхронизации результат записывается обратно в глобальную память.

Обе версии запускаются с одинаковой конфигурацией сетки и блоков, после чего сравнивается время выполнения.

---

### 2.3 Задание 2: Сложение массивов и влияние размера блока

Для сложения массивов реализовано CUDA-ядро `vector_add`, в котором каждый поток выполняет операцию:

c[idx] = a[idx] + b[idx];

Запуски выполняются с тремя различными размерами блока: 64, 128 и 256 потоков. Для каждого варианта измеряется и выводится время выполнения, что позволяет проанализировать влияние конфигурации блока на производительность.

---

### 2.4 Задание 3: Коалесцированный и некоалесцированный доступ

Реализованы два варианта CUDA-ядра:

- `coalesced_access` — потоки обращаются к соседним элементам массива, что обеспечивает коалесцированный доступ к глобальной памяти.
- `noncoalesced_access` — используется вычисление индекса `(idx * 32) % N`, что приводит к разрозненному, некоалесцированному доступу к памяти.

Обе версии выполняют одинаковую по смыслу операцию, но демонстрируют различие во времени выполнения из-за разного шаблона доступа к памяти.

---

### 2.5 Задание 4: Подбор конфигурации сетки и блоков

В качестве оптимальной конфигурации используется размер блока 256 потоков, который значительно лучше неоптимального варианта с 32 потоками, 
хотя в предыдущем эксперименте (задание 2) наилучший результат был получен при размере блока 128 потоков.

Для демонстрации влияния конфигурации используется ядро `coalesced_access`, которое запускается в двух вариантах:

- неоптимальная конфигурация: размер блока 32 потока;
- оптимизированная конфигурация: размер блока 256 потоков.

Сравнение времени выполнения показывает, что правильный выбор параметров запуска существенно влияет на производительность.

---

## 3. Организация и логика кода

В функции `main()`:

1. Создаются массивы на CPU с помощью `std::vector`.
2. Выделяется память на GPU с помощью `cudaMalloc`.
3. Данные копируются на GPU через `cudaMemcpy`.
4. Поочередно запускаются CUDA-ядра для всех четырех заданий.
5. Время выполнения измеряется с помощью CUDA Events.
6. После завершения работы память освобождается через `cudaFree`.

Для проверки ошибок используется вспомогательная функция `checkCuda`.

---

## 4. Методика измерения времени

Для измерения времени выполнения CUDA-ядер используется механизм CUDA Events:

- `cudaEventCreate`
- `cudaEventRecord`
- `cudaEventSynchronize`
- `cudaEventElapsedTime`

Для измерения времени выполнения в заданиях 3 используются отдельные функции measure_coalesced и measure_noncoalesced, 
которые инкапсулируют запуск соответствующих CUDA-ядер и измерение времени с помощью CUDA Events.

---

## 5. Анализ результатов экспериментов

В ходе выполнения работы были получены следующие экспериментальные результаты (время указано в миллисекундах):

- Задание 1:
    - Global memory: 0.165472 ms
    - Shared memory: 0.038912 ms
- Задание 2:
    - Block size 64: 0.067584 ms
    - Block size 128: 0.0256 ms
    - Block size 256: 0.022528 ms
- Задание 3:
    - Coalesced access: 0.031744 ms
    - Non-coalesced access: 0.156672 ms
- Задание 4:
    - Bad configuration (32 threads): 0.06656 ms
    - Good configuration (256 threads): 0.024576 ms

### Анализ по заданиям

**Задание 1 (Global vs Shared memory).**  
Время выполнения версии с использованием разделяемой памяти (0.038912 ms) существенно меньше, чем у версии, использующей только глобальную память (0.165472 ms). Ускорение составляет более чем 4 раза. Это объясняется тем, что доступ к shared memory имеет значительно меньшую задержку по сравнению с глобальной памятью, а также тем, что данные сначала загружаются в быстрый локальный буфер блока и затем используются оттуда.

**Задание 2 (Влияние размера блока).**  
Наилучшее время показал размер блока 256 потоков (0.022528 ms), немного опередив вариант с 128 потоками (0.0256 ms). Размер блока 64 потока работает заметно медленнее из-за недостаточной загрузки вычислительных ресурсов GPU и меньшего количества активных варпов на мультипроцессор. Результаты показывают, что увеличение размера блока улучшает заполняемость GPU и позволяет лучше скрывать задержки доступа к памяти, однако оптимальный размер блока определяется балансом между параллелизмом и использованием аппаратных ресурсов.

**Задание 3 (Коалесцированный и некоалесцированный доступ).**  
Коалесцированный доступ (0.031744 ms) выполняется почти в 5 раз быстрее, чем некоалесцированный (0.156672 ms). Это наглядно демонстрирует критическую важность шаблона доступа к глобальной памяти: при последовательном доступе потоки варпа объединяют обращения в минимальное число транзакций, тогда как при разрозненном доступе количество транзакций резко возрастает, что приводит к существенным потерям производительности.

**Задание 4 (Плохая и хорошая конфигурация).**  
Оптимизированная конфигурация с размером блока 256 потоков (0.024576 ms) работает почти в 3 раза быстрее, чем неоптимальная конфигурация с 32 потоками на блок (0.06656 ms). Это связано с тем, что при малом размере блока GPU не может эффективно загружать вычислительные ресурсы и скрывать задержки доступа к памяти из-за недостаточного числа активных варпов.

### Общий вывод по анализу

Полученные результаты наглядно показывают, что производительность CUDA-программ в решающей степени определяется:
- способом работы с памятью (global vs shared, коалесцированный доступ);
- правильно подобранной конфигурацией сетки и блоков потоков;
- уровнем параллелизма и эффективностью использования аппаратных ресурсов GPU.

Даже при одинаковом объеме вычислений разница в организации доступа к памяти и параметрах запуска может приводить к многократной разнице во времени выполнения.

---

## 6. Ответы на контрольные вопросы

### 1. Какие основные типы памяти существуют в архитектуре CUDA и чем они отличаются по скорости доступа?

В CUDA используются регистры, разделяемая память, глобальная память, а также константная и текстурная память. Самыми быстрыми являются регистры, затем следует разделяемая память, а глобальная память обладает наибольшей задержкой, но и наибольшим объемом.

---

### 2. В каких случаях использование разделяемой памяти позволяет ускорить выполнение CUDA-программы?

Когда данные используются несколькими потоками одного блока или переиспользуются несколько раз, имеет смысл сначала загрузить их в shared memory и работать с ними там, сокращая число обращений к глобальной памяти.

---

### 3. Как шаблон доступа к глобальной памяти влияет на производительность GPU-программы?

Если потоки одного варпа обращаются к соседним адресам памяти, доступ выполняется коалесцированно и за минимальное число транзакций. Разрозненный доступ приводит к большому числу обращений и снижению производительности.

---

### 4. Почему одинаковый алгоритм на GPU может показывать разное время выполнения при разных способах обращения к памяти?

Потому что в большинстве CUDA-программ время работы определяется не количеством арифметических операций, а эффективностью доступа к памяти.

---

### 5. Как размер блока потоков влияет на производительность CUDA-ядра?

Размер блока определяет количество активных варпов на мультипроцессоре, степень загрузки вычислительных ресурсов и использование регистров и разделяемой памяти.

---

### 6. Что такое варп и почему важно учитывать его при разработке CUDA-программ?

Варп — это группа из 32 потоков, которые выполняются одновременно. Ветвления внутри варпа приводят к последовательному выполнению разных веток и ухудшению производительности.

---

### 7. Какие факторы необходимо учитывать при выборе конфигурации сетки и блоков потоков?

Необходимо учитывать размер задачи, аппаратные ограничения GPU, использование регистров и разделяемой памяти, а также необходимость максимальной загрузки вычислительных блоков.

---

### 8. Почему оптимизация CUDA-программы часто начинается с анализа работы с памятью, а не с изменения алгоритма?

Потому что GPU выполняет вычисления очень быстро, и в большинстве случаев именно память является основным ограничивающим фактором производительности.

---
